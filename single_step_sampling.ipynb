{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0 x: x.shape=torch.Size([16, 1, 1024])\n",
      "\n",
      "level 1 x: x.shape=torch.Size([16, 4, 1024])\n",
      "\n",
      "level 2 x: x.shape=torch.Size([16, 9, 1024])\n",
      "\n",
      "level 3 x: x.shape=torch.Size([16, 16, 1024])\n",
      "\n",
      "level 4 x: x.shape=torch.Size([16, 25, 1024])\n",
      "\n",
      "level 5 x: x.shape=torch.Size([16, 36, 1024])\n",
      "\n",
      "level 6 x: x.shape=torch.Size([16, 64, 1024])\n",
      "\n",
      "level 7 x: x.shape=torch.Size([16, 100, 1024])\n",
      "\n",
      "level 8 x: x.shape=torch.Size([16, 169, 1024])\n",
      "\n",
      "level 9 x: x.shape=torch.Size([16, 256, 1024])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 412 #2137 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        # recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=1, top_p=0.15, g_seed=seed, more_smooth=more_smooth)\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "# f = vae.quant_conv(vae.encoder(x))\n",
    "# f_hat = torch.zeros_like(f)\n",
    "\n",
    "# for next_level in range(len(patch_nums)):\n",
    "#     current_level = next_level - 1\n",
    "\n",
    "#     print(f\"{next_level=}, patch_nums[next_level]={patch_nums[next_level]}\")\n",
    "\n",
    "#     h = var.autoregressive_single_step_prediction(\n",
    "#         current_level=current_level, \n",
    "#         f_hat=f_hat, \n",
    "#         label_B=label_B,\n",
    "#     ) \n",
    "\n",
    "#     f_hat = f_hat + h\n",
    "\n",
    "\n",
    "# image = vae.fhat_to_img(f_hat.detach()).clamp_(0, 1)\n",
    "\n",
    "# chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "# chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "# chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "# chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(f_hat):\n",
    "    image = vae.fhat_to_img(f_hat.detach()).add_(1).mul_(0.5)\n",
    "\n",
    "    chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "    chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "    chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "    chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "f_hat = torch.zeros_like(f)\n",
    "\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562) \n",
    "cfg = 4\n",
    "images = []\n",
    "token_maps = []\n",
    "\n",
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "f_hat = torch.zeros_like(f)\n",
    "batch_size = len(class_labels)\n",
    "\n",
    "label_B = torch.tensor(class_labels, device=device)\n",
    "step_token_map = var.get_initial_token_map(label_B)\n",
    "\n",
    "full_token_map = step_token_map.clone()\n",
    "\n",
    "\n",
    "class_conditioning = var.get_class_conditioning(label_B)\n",
    "\n",
    "for map_size_index, patch_num, in enumerate(patch_nums):\n",
    "    # print(f\"{map_size_index=}, {patch_num=}\")\n",
    "\n",
    "    if map_size_index != 0:\n",
    "        step_token_map = var.prepare_token_map(f_hat, map_size_index=map_size_index)\n",
    "        full_token_map = torch.cat([full_token_map, step_token_map], dim=1)\n",
    "\n",
    "    level_ratio = map_size_index / (len(patch_nums) - 1)\n",
    "\n",
    "    logits = var.token_map_to_relevant_logits(full_token_map.clone(), class_conditioning, map_size_index=map_size_index, masked=True)\n",
    "\n",
    "    indices = var.logits_to_indices(logits, cfg=cfg, batch_size=batch_size, level_ratio=level_ratio)\n",
    "    h = var.indices_to_h(indices, batch_size=batch_size, level_ratio=level_ratio, patch_num=patch_num)\n",
    "\n",
    "    f_hat = f_hat + h\n",
    "\n",
    "    images.append(vae.fhat_to_img(f_hat.detach()).add_(1).mul_(0.5))\n",
    "\n",
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "# visualise(f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's make it a single_step inference\n",
    "\n",
    "We need to make the whole full_token_map at a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "f_hat = torch.zeros_like(f)\n",
    "\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562) \n",
    "cfg = 5\n",
    "top_k=50\n",
    "top_p=0.4\n",
    "\n",
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "f_hat = torch.zeros_like(f)\n",
    "batch_size = len(class_labels)\n",
    "\n",
    "label_B = torch.tensor(class_labels, device=device)\n",
    "class_conditioning = var.get_class_conditioning(label_B)\n",
    "\n",
    "\n",
    "def get_full_token_map(map_size_index, label_B, f_hat, last_grad_only=True):\n",
    "    full_token_map = var.get_initial_token_map(label_B)\n",
    "    \n",
    "    if map_size_index == 0:\n",
    "        return full_token_map\n",
    "    \n",
    "    with torch.no_grad() if last_grad_only else torch.enable_grad():\n",
    "        for i in range(1, map_size_index): # for each level up to the second to last level\n",
    "            step_token_map = var.prepare_token_map(f_hat, map_size_index=i)\n",
    "            full_token_map = torch.cat([full_token_map, step_token_map], dim=1)\n",
    "\n",
    "    # for the last level we need grad anyway\n",
    "    last_step_token_map = var.prepare_token_map(f_hat, map_size_index=map_size_index) \n",
    "    full_token_map = torch.cat([full_token_map, last_step_token_map], dim=1)\n",
    "\n",
    "    return full_token_map\n",
    "\n",
    "\n",
    "for map_size_index, patch_num, in enumerate(patch_nums):\n",
    "    full_token_map = get_full_token_map(map_size_index, label_B, f_hat, last_grad_only=True)\n",
    "\n",
    "    level_ratio = map_size_index / (len(patch_nums) - 1)\n",
    "\n",
    "    logits = var.token_map_to_relevant_logits(full_token_map.clone(), class_conditioning, map_size_index=map_size_index, masked=True)\n",
    "\n",
    "    indices = var.logits_to_indices(logits, cfg=cfg, batch_size=batch_size, level_ratio=level_ratio, top_k=top_k, top_p=top_p)\n",
    "    h = var.indices_to_h(indices, batch_size=batch_size, level_ratio=level_ratio, patch_num=patch_num)\n",
    "\n",
    "    f_hat = f_hat + h\n",
    "\n",
    "\n",
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "visualise(f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n"
     ]
    }
   ],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "f_hat = torch.zeros_like(f)\n",
    "\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562) \n",
    "cfg = 4\n",
    "\n",
    "f_hat = torch.zeros_like(f)\n",
    "label_B = torch.tensor(class_labels, device=device)\n",
    "\n",
    "for map_size_index, patch_num, in enumerate(patch_nums):\n",
    "    f_hat += var.predict_single_step_residual(f_hat, label_B, map_size_index=map_size_index, cfg=cfg)    \n",
    "\n",
    "# visualise(f_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# And now the optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_size_index = -1 # last patch num that will be included\n",
    "\n",
    "quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, map_size_index)\n",
    "\n",
    "token_map = vae.quantize.limited_quant_pyramid_to_var_input(quant_pyramid) # omits the class conditioning\n",
    "# len(quant_pyramid)\n",
    "\n",
    "# vae.quantize.limited_quant_pyramid_to_var_input\n",
    "# var.predict_single_step_from_quant_pyramid(quant_pyramid=quant_pyramid, label_B=label_B, cfg=cfg).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.quant_pyramid_to_var_inputs(quant_pyramid, label_B, batch_size=8).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# var.predict_single_step_from_quant_pyramid(quant_pyramid=quant_pyramid, label_B=label_B, cfg=cfg).shape\n",
    "var.predict_single_step_from_quant_pyramid(quant_pyramid=[], label_B=label_B, cfg=cfg, batch_size=8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_size_index=0, 1x1, current_length=1\n",
      "map_size_index=1, 2x2, current_length=5\n",
      "map_size_index=2, 3x3, current_length=14\n",
      "map_size_index=3, 4x4, current_length=30\n",
      "map_size_index=4, 5x5, current_length=55\n",
      "map_size_index=5, 6x6, current_length=91\n",
      "map_size_index=6, 8x8, current_length=155\n",
      "map_size_index=7, 10x10, current_length=255\n",
      "map_size_index=8, 13x13, current_length=424\n",
      "map_size_index=9, 16x16, current_length=680\n"
     ]
    }
   ],
   "source": [
    "for map_size_index in range(len(patch_nums)):\n",
    "    current_length = sum([\n",
    "        patch_size * patch_size\n",
    "        for patch_size\n",
    "        in patch_nums[:map_size_index + 1]\n",
    "    ])  \n",
    "\n",
    "    patch_size = patch_nums[map_size_index]\n",
    "    print(f\"{map_size_index=}, {patch_size}x{patch_size}, {current_length=}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "def downsample_residual(residual, patch_size):\n",
    "    if patch_size == patch_nums[-1]:\n",
    "        return residual\n",
    "\n",
    "    return F.interpolate(residual, size=(patch_size, patch_size), mode='area')\n",
    "\n",
    "\n",
    "def get_optimization_loss(f, predicted_map_size_index, batch_size=8, cfg=4, top_k=900, top_p=0.95, scale_by_inv_area=False, seed=None):\n",
    "    f_hat_map_size_index = predicted_map_size_index - 1\n",
    "    predicted_map_size = patch_nums[predicted_map_size_index]\n",
    "\n",
    "    rng = None\n",
    "\n",
    "    if seed is not None:\n",
    "        rng = torch.Generator(device='cuda')\n",
    "        rng.manual_seed(seed)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, f_hat_map_size_index) \n",
    "        predicted_residual = var.predict_single_step_from_quant_pyramid(quant_pyramid=quant_pyramid, label_B=label_B, cfg=cfg, batch_size=batch_size, top_k=top_k, top_p=top_p, rng=rng)\n",
    "\n",
    "    real_residual = f - f_hat\n",
    "\n",
    "    loss = F.mse_loss(\n",
    "        downsample_residual(predicted_residual, predicted_map_size), \n",
    "        downsample_residual(real_residual, predicted_map_size)\n",
    "    )\n",
    "\n",
    "    if scale_by_inv_area:\n",
    "        loss /= predicted_map_size * predicted_map_size\n",
    "\n",
    "\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_82206/2341518544.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=True, device=device)\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "100%|██████████| 500/500 [00:15<00:00, 31.51it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# f = torch.randn([4, 32, 16, 16], requires_grad=True, device=device) \n",
    "# class_labels = (980, 437, 22, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "\n",
    "img = recon_B3HW.float().clone().clamp_(0, 1).to(device)\n",
    "f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=True, device=device)\n",
    "f = torch.zeros_like(f, requires_grad=True, device=device)\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "cfg = 4\n",
    "top_k = 500 #700\n",
    "top_p = 0.25 #0.95 #0.95\n",
    "lr = 0.2\n",
    "scale_by_inv_area = False\n",
    "\n",
    "\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=lr)\n",
    "total_steps = 500\n",
    "\n",
    "min_next_map_size_index = 0 #len(patch_nums) - 2 #-1 #len(patch_nums) - 4\n",
    "max_next_map_size_index = len(patch_nums) - 1 \n",
    "\n",
    "\n",
    "for i in tqdm(range(total_steps), total=total_steps):\n",
    "    next_map_size_index = np.random.randint(min_next_map_size_index, max_next_map_size_index + 1)\n",
    "\n",
    "    loss = get_optimization_loss(f, next_map_size_index, batch_size=8, cfg=cfg, top_k=top_k, top_p=top_p, scale_by_inv_area=scale_by_inv_area)\n",
    "\n",
    "    \n",
    "    # patch_weight_bias = .001\n",
    "    # patch_grad_scale = (patch_nums[len(patch_nums)-1]**2 + patch_weight_bias)  / (patch_nums[next_map_size_index]**2 + patch_weight_bias)\n",
    "    # loss*= np.sqrt(patch_grad_scale)\n",
    "    \n",
    "    # step_weight = (total_steps - i) / total_steps\n",
    "    # loss*= step_weight\n",
    "\n",
    "    # level_ratio = (next_map_size_index + 1)/ (len(patch_nums)) \n",
    "    # level_weight = np.power(level_ratio, 1/4)\n",
    "    # loss*= level_weight\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "f = f.detach()\n",
    "visualise(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:01<00:00, 246.47it/s]\n",
      "100%|██████████| 250/250 [00:01<00:00, 210.58it/s]\n",
      "100%|██████████| 250/250 [00:01<00:00, 173.84it/s]\n",
      "100%|██████████| 250/250 [00:01<00:00, 134.30it/s]\n",
      "100%|██████████| 250/250 [00:02<00:00, 91.47it/s]\n",
      "100%|██████████| 250/250 [00:04<00:00, 58.96it/s]\n",
      "100%|██████████| 250/250 [00:07<00:00, 35.44it/s]\n",
      "100%|██████████| 250/250 [00:09<00:00, 26.23it/s]\n",
      "100%|██████████| 250/250 [00:17<00:00, 14.10it/s]\n",
      "100%|██████████| 250/250 [00:31<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.coreaddons: Expected a KPluginFactory, got a KIOPluginForMetaData\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# f = torch.zeros_like(f, requires_grad=True, device=device)\n",
    "\n",
    "\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "initial_noise = 0.1\n",
    "cfg = 7\n",
    "top_k = 600 #700\n",
    "top_p = 0.65 #0.95 #0.95\n",
    "lr = 0.02\n",
    "steps_per_size = 250\n",
    "seed = 42\n",
    "\n",
    "scale_by_inv_area = False\n",
    "\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "\n",
    "f_initial = torch.randn_like(f, device=device) * initial_noise # std=0.5\n",
    "f = f_initial.clone().detach().requires_grad_(True)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=lr)\n",
    "\n",
    "for max_next_map_size_index in range(len(patch_nums)):\n",
    "\n",
    "    for i in tqdm(range(steps_per_size), total=steps_per_size):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # max_next_map_size_index = int(np.sqrt(i/total_steps) * len(patch_nums)) \n",
    "        \n",
    "        loss = 0\n",
    "        loss += get_optimization_loss(f, max_next_map_size_index, batch_size=8, cfg=cfg, top_k=top_k, top_p=top_p, scale_by_inv_area=scale_by_inv_area, seed=seed)\n",
    "\n",
    "        random_size_index = np.random.randint(0, max_next_map_size_index + 1)\n",
    "        loss += get_optimization_loss(f, random_size_index, batch_size=8, cfg=cfg, top_k=top_k, top_p=top_p, scale_by_inv_area=scale_by_inv_area, seed=seed)    \n",
    "\n",
    "        # if i % 100 == 0:\n",
    "        #     print(f'loss: {loss.item()}')\n",
    "\n",
    "        step_weight = np.power((total_steps - i + 1) / total_steps, 1/4)\n",
    "\n",
    "        loss*= step_weight\n",
    "        \n",
    "\n",
    "        #loss*= step_weight\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "visualise(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6520, device='cuda:0', grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 91, 1024])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(token_maps, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "self=var\n",
    "map_size_index = map_size_id\n",
    "\n",
    "current_length = sum([\n",
    "    patch_size * patch_size\n",
    "    for patch_size\n",
    "    in self.patch_nums[:map_size_index + 1]\n",
    "])\n",
    "\n",
    "full_token_map = torch.cat(token_maps, dim=1).clone()\n",
    "\n",
    "output_length = self.patch_nums[map_size_index] ** 2\n",
    "attn_bias = self.attn_bias_for_masking[:, :, :current_length, :current_length]\n",
    "\n",
    "kv_pairs = []\n",
    "\n",
    "for b in self.blocks:\n",
    "    token_map = b(x=full_token_map, cond_BD=class_conditioning, attn_bias=attn_bias)#, attn_bias=attn_bias)\n",
    "    kv_pairs.append({\n",
    "        'key': b.attn.cached_k,\n",
    "        'value': b.attn.cached_v\n",
    "    })\n",
    "\n",
    "self.last_kv_pairs = kv_pairs\n",
    "\n",
    "output_length = self.patch_nums[map_size_index] ** 2\n",
    "\n",
    "relevant_output_tokens = token_map[:, -output_length:]\n",
    "\n",
    "logits = self.get_logits(relevant_output_tokens, class_conditioning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 36, 4096])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_nums[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vae.fhat_to_img(f_hat.detach()).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.0807, device='cuda:0'), tensor(2.9216, device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_hat.min(), f_hat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "images_stack = torch.cat(images, dim=0)\n",
    "\n",
    "\n",
    "chw = torchvision.utils.make_grid(images_stack, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.attn_bias_for_masking[0][0][:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_bias_for_masking(var, map_size_index):\n",
    "    input_length = var.patch_nums[map_size_index] ** 2\n",
    "    attn_bias = var.attn_bias_for_masking[:, :, :input_length, :input_length]\n",
    "    return attn_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attn_bias_for_masking(var, 1)[0][0][:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 14, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_pairs[2][0]['key'].shape\n",
    "\n",
    "# 16 x 16 x length x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_size_index=0, current_length=1\n",
      "map_size_index=1, current_length=5\n",
      "map_size_index=2, current_length=14\n",
      "map_size_index=3, current_length=30\n",
      "map_size_index=4, current_length=55\n",
      "map_size_index=5, current_length=91\n",
      "map_size_index=6, current_length=155\n",
      "map_size_index=7, current_length=255\n",
      "map_size_index=8, current_length=424\n",
      "map_size_index=9, current_length=680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.coreaddons: Expected a KPluginFactory, got a KIOPluginForMetaData\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for map_size_index, patch_num in enumerate(patch_nums):\n",
    "    current_length = sum([\n",
    "                patch_size * patch_size\n",
    "                for patch_size\n",
    "                in patch_nums[:map_size_index + 1]\n",
    "            ])\n",
    "    print(f\"map_size_index={map_size_index}, current_length={current_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-var",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
