{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0 x: x.shape=torch.Size([16, 1, 1024])\n",
      "\n",
      "level 1 x: x.shape=torch.Size([16, 4, 1024])\n",
      "\n",
      "level 2 x: x.shape=torch.Size([16, 9, 1024])\n",
      "\n",
      "level 3 x: x.shape=torch.Size([16, 16, 1024])\n",
      "\n",
      "level 4 x: x.shape=torch.Size([16, 25, 1024])\n",
      "\n",
      "level 5 x: x.shape=torch.Size([16, 36, 1024])\n",
      "\n",
      "level 6 x: x.shape=torch.Size([16, 64, 1024])\n",
      "\n",
      "level 7 x: x.shape=torch.Size([16, 100, 1024])\n",
      "\n",
      "level 8 x: x.shape=torch.Size([16, 169, 1024])\n",
      "\n",
      "level 9 x: x.shape=torch.Size([16, 256, 1024])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 1 #2137 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_level=0, patch_nums[next_level]=1\n",
      "next_level=1, patch_nums[next_level]=2\n",
      "current_level=0 cur_L: cur_L=0\n",
      "next_level=2, patch_nums[next_level]=3\n",
      "current_level=1 cur_L: cur_L=1\n",
      "next_level=3, patch_nums[next_level]=4\n",
      "current_level=2 cur_L: cur_L=5\n",
      "next_level=4, patch_nums[next_level]=5\n",
      "current_level=3 cur_L: cur_L=14\n",
      "next_level=5, patch_nums[next_level]=6\n",
      "current_level=4 cur_L: cur_L=30\n",
      "next_level=6, patch_nums[next_level]=8\n",
      "current_level=5 cur_L: cur_L=55\n",
      "next_level=7, patch_nums[next_level]=10\n",
      "current_level=6 cur_L: cur_L=91\n",
      "next_level=8, patch_nums[next_level]=13\n",
      "current_level=7 cur_L: cur_L=155\n",
      "next_level=9, patch_nums[next_level]=16\n",
      "current_level=8 cur_L: cur_L=255\n"
     ]
    }
   ],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "f_hat = torch.zeros_like(f)\n",
    "\n",
    "for next_level in range(len(patch_nums)):\n",
    "    current_level = next_level - 1\n",
    "\n",
    "    print(f\"{next_level=}, patch_nums[next_level]={patch_nums[next_level]}\")\n",
    "\n",
    "    h = var.autoregressive_single_step_prediction(\n",
    "        current_level=current_level, \n",
    "        f_hat=f_hat, \n",
    "        label_B=label_B,\n",
    "    ) \n",
    "\n",
    "    f_hat = f_hat + h\n",
    "\n",
    "\n",
    "image = vae.fhat_to_img(f_hat.detach()).clamp_(0, 1)\n",
    "\n",
    "# chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "# chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "# chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "# chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(f_hat):\n",
    "    image = vae.fhat_to_img(f_hat.detach()).add_(1).mul_(0.5)\n",
    "\n",
    "    chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "    chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "    chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "    chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_size_index=0, level_ratio=0.0\n",
      "map_size_index=1, level_ratio=0.1111111111111111\n",
      "map_size_index=2, level_ratio=0.2222222222222222\n",
      "map_size_index=3, level_ratio=0.3333333333333333\n",
      "map_size_index=4, level_ratio=0.4444444444444444\n",
      "map_size_index=5, level_ratio=0.5555555555555556\n",
      "map_size_index=6, level_ratio=0.6666666666666666\n",
      "map_size_index=7, level_ratio=0.7777777777777778\n",
      "map_size_index=8, level_ratio=0.8888888888888888\n",
      "map_size_index=9, level_ratio=1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.coreaddons: Expected a KPluginFactory, got a KIOPluginForMetaData\n"
     ]
    }
   ],
   "source": [
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562) \n",
    "cfg = 4\n",
    "images = []\n",
    "token_maps = []\n",
    "\n",
    "kv_pairs = []\n",
    "attention_maps = []\n",
    "\n",
    "f_hat = torch.zeros_like(f)\n",
    "batch_size = len(class_labels)\n",
    "\n",
    "label_B = torch.tensor(class_labels, device=device)\n",
    "token_map = var.get_initial_token_map(label_B)\n",
    "\n",
    "\n",
    "class_conditioning = var.get_class_conditioning(label_B)\n",
    "\n",
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "for map_size_index, patch_num, in enumerate(patch_nums):\n",
    "\n",
    "    \n",
    "    # if map_size_index > 7:\n",
    "    #     for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "    # if map_size_index % 3 == 0:\n",
    "    #     visualise(f_hat)\n",
    "\n",
    "    if map_size_index != 0:\n",
    "        token_map = var.prepare_token_map(f_hat, map_size_index=map_size_index)\n",
    "\n",
    "    level_ratio = map_size_index / (len(patch_nums) - 1)\n",
    "\n",
    "    print(f\"{map_size_index=}, {level_ratio=}\")\n",
    "\n",
    "    token_maps.append(token_map)\n",
    "    # for b in var.blocks: b.attn.kv_caching(True)\n",
    "\n",
    "    logits = var.token_map_to_logits(token_map, class_conditioning, map_size_index=map_size_index)\n",
    "    kv_pairs.append(var.last_kv_pairs)\n",
    "    indices = var.logits_to_indices(logits, cfg=cfg, batch_size=batch_size, level_ratio=level_ratio) # TODO: remove rng\n",
    "    h = var.indices_to_h(indices, batch_size=batch_size, level_ratio=level_ratio, patch_num=patch_num)\n",
    "\n",
    "    f_hat = f_hat + h\n",
    "\n",
    "    images.append(vae.fhat_to_img(f_hat.detach()).add_(1).mul_(0.5))\n",
    "\n",
    "for b in var.blocks: b.attn.kv_caching(False)\n",
    "\n",
    "visualise(f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vae.fhat_to_img(f_hat.detach()).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-3.0807, device='cuda:0'), tensor(2.9216, device='cuda:0'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_hat.min(), f_hat.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "images_stack = torch.cat(images, dim=0)\n",
    "\n",
    "\n",
    "chw = torchvision.utils.make_grid(images_stack, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 1024])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., -inf, -inf, -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.attn_bias_for_masking[0][0][:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_bias_for_masking(var, map_size_index):\n",
    "    input_length = var.patch_nums[map_size_index] ** 2\n",
    "    attn_bias = var.attn_bias_for_masking[:, :, :input_length, :input_length]\n",
    "    return attn_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_attn_bias_for_masking(var, 1)[0][0][:10,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 256, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_maps[-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 16, 14, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kv_pairs[2][0]['key'].shape\n",
    "\n",
    "# 16 x 16 x length x 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_size_index=0, current_length=1\n",
      "map_size_index=1, current_length=5\n",
      "map_size_index=2, current_length=14\n",
      "map_size_index=3, current_length=30\n",
      "map_size_index=4, current_length=55\n",
      "map_size_index=5, current_length=91\n",
      "map_size_index=6, current_length=155\n",
      "map_size_index=7, current_length=255\n",
      "map_size_index=8, current_length=424\n",
      "map_size_index=9, current_length=680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.coreaddons: Expected a KPluginFactory, got a KIOPluginForMetaData\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for map_size_index, patch_num in enumerate(patch_nums):\n",
    "    current_length = sum([\n",
    "                patch_size * patch_size\n",
    "                for patch_size\n",
    "                in patch_nums[:map_size_index + 1]\n",
    "            ])\n",
    "    print(f\"map_size_index={map_size_index}, current_length={current_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-var",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
