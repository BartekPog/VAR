{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ðŸš€ For an interactive experience, head over to our [demo platform](https://var.vision/demo) and dive right in! ðŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bpogodzi/code/var/venv-var/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next_token_map: next_token_map.shape=torch.Size([16, 1, 1024])\n",
      " next_token_map.dtype=torch.float32\n",
      " next_token_map.device=device(type='cuda', index=0)\n",
      "\n",
      "[autoregressive_infer_cfg] f_hat.shape=torch.Size([8, 32, 16, 16])\n",
      ", next_token_map.shape=torch.Size([16, 1, 1024])\n",
      "\n",
      "level 0 x: x.shape=torch.Size([16, 1, 1024])\n",
      "\n",
      "level 1 x: x.shape=torch.Size([16, 4, 1024])\n",
      "\n",
      "level 2 x: x.shape=torch.Size([16, 9, 1024])\n",
      "\n",
      "level 3 x: x.shape=torch.Size([16, 16, 1024])\n",
      "\n",
      "level 4 x: x.shape=torch.Size([16, 25, 1024])\n",
      "\n",
      "level 5 x: x.shape=torch.Size([16, 36, 1024])\n",
      "\n",
      "level 6 x: x.shape=torch.Size([16, 64, 1024])\n",
      "\n",
      "level 7 x: x.shape=torch.Size([16, 100, 1024])\n",
      "\n",
      "level 8 x: x.shape=torch.Size([16, 169, 1024])\n",
      "\n",
      "level 9 x: x.shape=torch.Size([16, 256, 1024])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bpogodzi/code/var/venv-var/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    }
   ],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 2137 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1} # IRRELEVANT -- variable not used\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "# chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "# chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "# chw.show()\n",
    "# recon_B3HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimized_image = torch.rand_like(recon_B3HW, requires_grad=True)\n",
    "# image_pyramid = vae.img_to_idxBl(optimized_image, patch_nums)\n",
    "\n",
    "# image_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed = vae.idxBl_to_img(image_pyramid, same_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed[6].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_B3HW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n"
     ]
    }
   ],
   "source": [
    "# x = recon_B3HW\n",
    "# x = torch.rand(8, 3, 256, 256, device=device)\n",
    "\n",
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "\n",
    "image = vae.fhat_to_img(f)\n",
    "f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "image = vae.fhat_to_img(f)\n",
    "\n",
    "# x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "# f = vae.quant_conv(vae.encoder(x))\n",
    "# image = vae.fhat_to_img(f).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n",
    "\n",
    "# ls_f_hat_BChw = vae.quantize.f_to_idxBl_or_fhat(f, to_fhat=False, v_patch_nums=patch_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "last_patch_id = -1\n",
    "\n",
    "quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "residual = f - f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(quant_pyramid)=0)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(quant_pyramid)=})\")\n",
    "if quant_pyramid:\n",
    "    print(f\"{quant_pyramid[0].shape=}\")\n",
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the pyramid as the input for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=-1, next_level=0\n",
      "next_token_map lvl current_level=-1 next_level=0: next_token_map.shape=torch.Size([16, 1, 1024])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_residual = var.autoregressive_single_step_prediction(\n",
    "    quant_pyramid=quant_pyramid, \n",
    "    f_hat=f_hat, \n",
    "    label_B=label_B,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_residual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation POC in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimisation_loss(f, last_patch_id):\n",
    "    quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "    f_residual = f - f_hat\n",
    "\n",
    "    predicted_residual = var.autoregressive_single_step_prediction(\n",
    "        quant_pyramid=quant_pyramid, \n",
    "        f_hat=f_hat, \n",
    "        label_B=label_B, \n",
    "        cfg=cfg, \n",
    "        top_k=900, \n",
    "        top_p=0.95)\n",
    "\n",
    "    return F.mse_loss(predicted_residual, f_residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=-1, next_level=0\n",
      "next_token_map lvl current_level=-1 next_level=0: next_token_map.shape=torch.Size([4, 1, 1024])\n",
      "\n",
      "coarsness_step: -1, loss: 0.39253973960876465\n",
      "current_level=0, next_level=1\n",
      "next_token_map lvl current_level=0 next_level=1: next_token_map.shape=torch.Size([4, 4, 1024])\n",
      "\n",
      "coarsness_step: 0, loss: 0.0009515469428151846\n",
      "current_level=1, next_level=2\n",
      "next_token_map lvl current_level=1 next_level=2: next_token_map.shape=torch.Size([4, 9, 1024])\n",
      "\n",
      "coarsness_step: 1, loss: 0.002446998842060566\n",
      "current_level=2, next_level=3\n",
      "next_token_map lvl current_level=2 next_level=3: next_token_map.shape=torch.Size([4, 16, 1024])\n",
      "\n",
      "coarsness_step: 2, loss: 0.004130854737013578\n",
      "current_level=3, next_level=4\n",
      "next_token_map lvl current_level=3 next_level=4: next_token_map.shape=torch.Size([4, 25, 1024])\n",
      "\n",
      "coarsness_step: 3, loss: 0.006683871150016785\n",
      "current_level=4, next_level=5\n",
      "next_token_map lvl current_level=4 next_level=5: next_token_map.shape=torch.Size([4, 36, 1024])\n",
      "\n",
      "coarsness_step: 4, loss: 0.009225272573530674\n",
      "current_level=5, next_level=6\n",
      "next_token_map lvl current_level=5 next_level=6: next_token_map.shape=torch.Size([4, 64, 1024])\n",
      "\n",
      "coarsness_step: 5, loss: 0.01421268004924059\n",
      "current_level=6, next_level=7\n",
      "next_token_map lvl current_level=6 next_level=7: next_token_map.shape=torch.Size([4, 100, 1024])\n",
      "\n",
      "coarsness_step: 6, loss: 0.02462838962674141\n",
      "current_level=7, next_level=8\n",
      "next_token_map lvl current_level=7 next_level=8: next_token_map.shape=torch.Size([4, 169, 1024])\n",
      "\n",
      "coarsness_step: 7, loss: 0.04235750064253807\n",
      "current_level=8, next_level=9\n",
      "next_token_map lvl current_level=8 next_level=9: next_token_map.shape=torch.Size([4, 256, 1024])\n",
      "\n",
      "coarsness_step: 8, loss: 0.10407813638448715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = torch.rand([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (22, 562)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.1)\n",
    "\n",
    "steps_per_coarsness = 1\n",
    "\n",
    "for coarsness_step in range(-1, len((patch_nums)) - 1):\n",
    "    for i in range(steps_per_coarsness):\n",
    "        optimizer.zero_grad()\n",
    "        loss = get_optimisation_loss(f, coarsness_step)\n",
    "        patch_weight = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "        loss*= patch_weight * step_weight\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.patch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vae.fhat_to_img(f).add_(1).mul_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random coarsness step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_179735/2704800417.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f = torch.tensor(vae.quant_conv(vae.encoder(img)), requires_grad=True, device=device)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=8, next_level=9\n",
      "next_token_map lvl current_level=8 next_level=9: next_token_map.shape=torch.Size([16, 256, 1024])\n",
      "\n",
      "level_ratio=0.9090909090909091, level_weight=0.9534625892455924, step_weight=1.0, patch_grad_scale=0.66015625, loss.item()=0.2066161185503006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# f = torch.randn([4, 32, 16, 16], requires_grad=True, device=device) \n",
    "# class_labels = (980, 437, 22, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "\n",
    "img = recon_B3HW.float()\n",
    "f = torch.tensor(vae.quant_conv(vae.encoder(img)), requires_grad=True, device=device)\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.00001)\n",
    "total_steps = 1\n",
    "\n",
    "min_coarsness = len(patch_nums) - 2\n",
    "max_coarsness = len(patch_nums) - 1\n",
    "\n",
    "for i in tqdm(range(total_steps), total=total_steps):\n",
    "    coarsness_step = np.random.randint(min_coarsness, max_coarsness)\n",
    "\n",
    "    loss = get_optimisation_loss(f, coarsness_step)\n",
    "\n",
    "    patch_grad_scale = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "    loss*= patch_grad_scale\n",
    "    \n",
    "    step_weight = (total_steps - i) / total_steps\n",
    "    loss*= step_weight\n",
    "\n",
    "    level_ratio = (coarsness_step + 2)/ (len(patch_nums) + 1) \n",
    "    level_weight = np.sqrt(level_ratio)\n",
    "    loss*= level_weight\n",
    "\n",
    "    print(f'{level_ratio=}, {level_weight=}, {step_weight=}, {patch_grad_scale=}, {loss.item()=}')\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "f = f.detach()\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach())#.add_(1).mul_(0.5)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patch_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-var",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
