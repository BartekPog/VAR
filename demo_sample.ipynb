{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ðŸš€ For an interactive experience, head over to our [demo platform](https://var.vision/demo) and dive right in! ðŸŒŸ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/BS/var-training/work/var/venv-var/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[constructor]  ==== flash_if_available=True (0/16), fused_if_available=True (fusing_add_ln=0/16, fusing_mlp=0/16) ==== \n",
      "    [VAR config ] embed_dim=1024, num_heads=16, depth=16, mlp_ratio=4.0\n",
      "    [drop ratios ] drop_rate=0.0, attn_drop_rate=0.0, drop_path_rate=0.0666667 (tensor([0.0000, 0.0044, 0.0089, 0.0133, 0.0178, 0.0222, 0.0267, 0.0311, 0.0356,\n",
      "        0.0400, 0.0444, 0.0489, 0.0533, 0.0578, 0.0622, 0.0667]))\n",
      "\n",
      "[init_weights] VAR with init_std=0.0180422\n",
      "prepare finished.\n"
     ]
    }
   ],
   "source": [
    "################## 1. Download checkpoints and build models\n",
    "import os\n",
    "import os.path as osp\n",
    "import torch, torchvision\n",
    "import random\n",
    "import numpy as np\n",
    "import PIL.Image as PImage, PIL.ImageDraw as PImageDraw\n",
    "setattr(torch.nn.Linear, 'reset_parameters', lambda self: None)     # disable default parameter init for faster speed\n",
    "setattr(torch.nn.LayerNorm, 'reset_parameters', lambda self: None)  # disable default parameter init for faster speed\n",
    "from models import VQVAE, build_vae_var\n",
    "\n",
    "MODEL_DEPTH = 16    # TODO: =====> please specify MODEL_DEPTH <=====\n",
    "assert MODEL_DEPTH in {16, 20, 24, 30}\n",
    "\n",
    "\n",
    "# download checkpoint\n",
    "hf_home = 'https://huggingface.co/FoundationVision/var/resolve/main'\n",
    "vae_ckpt, var_ckpt = 'vae_ch160v4096z32.pth', f'var_d{MODEL_DEPTH}.pth'\n",
    "if not osp.exists(vae_ckpt): os.system(f'wget {hf_home}/{vae_ckpt}')\n",
    "if not osp.exists(var_ckpt): os.system(f'wget {hf_home}/{var_ckpt}')\n",
    "\n",
    "# build vae, var\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if 'vae' not in globals() or 'var' not in globals():\n",
    "    vae, var = build_vae_var(\n",
    "        V=4096, Cvae=32, ch=160, share_quant_resi=4,    # hard-coded VQVAE hyperparameters\n",
    "        device=device, patch_nums=patch_nums,\n",
    "        num_classes=1000, depth=MODEL_DEPTH, shared_aln=False,\n",
    "    )\n",
    "\n",
    "# load checkpoints\n",
    "vae.load_state_dict(torch.load(vae_ckpt, map_location='cpu'), strict=True)\n",
    "var.load_state_dict(torch.load(var_ckpt, map_location='cpu'), strict=True)\n",
    "vae.eval(), var.eval()\n",
    "for p in vae.parameters(): p.requires_grad_(False)\n",
    "for p in var.parameters(): p.requires_grad_(False)\n",
    "print(f'prepare finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level 0 x: x.shape=torch.Size([16, 1, 1024])\n",
      "\n",
      "level 1 x: x.shape=torch.Size([16, 4, 1024])\n",
      "\n",
      "level 2 x: x.shape=torch.Size([16, 9, 1024])\n",
      "\n",
      "level 3 x: x.shape=torch.Size([16, 16, 1024])\n",
      "\n",
      "level 4 x: x.shape=torch.Size([16, 25, 1024])\n",
      "\n",
      "level 5 x: x.shape=torch.Size([16, 36, 1024])\n",
      "\n",
      "level 6 x: x.shape=torch.Size([16, 64, 1024])\n",
      "\n",
      "level 7 x: x.shape=torch.Size([16, 100, 1024])\n",
      "\n",
      "level 8 x: x.shape=torch.Size([16, 169, 1024])\n",
      "\n",
      "level 9 x: x.shape=torch.Size([16, 256, 1024])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############################# 2. Sample with classifier-free guidance\n",
    "\n",
    "# set args\n",
    "seed = 2137 #@param {type:\"number\"}\n",
    "torch.manual_seed(seed)\n",
    "num_sampling_steps = 250 #@param {type:\"slider\", min:0, max:1000, step:1} # IRRELEVANT -- variable not used\n",
    "cfg = 4 #@param {type:\"slider\", min:1, max:10, step:0.1}\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "more_smooth = False # True for more smooth output\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# run faster\n",
    "tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = bool(tf32)\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(tf32)\n",
    "torch.set_float32_matmul_precision('high' if tf32 else 'highest')\n",
    "\n",
    "# sample\n",
    "B = len(class_labels)\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "with torch.inference_mode():\n",
    "    with torch.autocast('cuda', enabled=True, dtype=torch.float16, cache_enabled=True):    # using bfloat16 can be faster\n",
    "        recon_B3HW = var.autoregressive_infer_cfg(B=B, label_B=label_B, cfg=cfg, top_k=900, top_p=0.95, g_seed=seed, more_smooth=more_smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "# chw = torchvision.utils.make_grid(recon_B3HW, nrow=8, padding=0, pad_value=1.0)\n",
    "# chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "# chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "# chw.show()\n",
    "# recon_B3HW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "# image_pyramid = vae.img_to_idxBl(optimized_image, patch_nums)\n",
    "\n",
    "# image_pyramid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed = vae.idxBl_to_img(image_pyramid, same_shape=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstructed[6].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 3, 256, 256])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_B3HW.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "# x = recon_B3HW\n",
    "# x = torch.rand(8, 3, 256, 256, device=device)\n",
    "\n",
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "# image = vae.fhat_to_img(f)\n",
    "# f = vae.quant_conv(vae.encoder(image))\n",
    "\n",
    "image = vae.fhat_to_img(f).clamp_(0, 1)\n",
    "\n",
    "# x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "# f = vae.quant_conv(vae.encoder(x))\n",
    "# image = vae.fhat_to_img(f).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n",
    "\n",
    "# ls_f_hat_BChw = vae.quantize.f_to_idxBl_or_fhat(f, to_fhat=False, v_patch_nums=patch_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "f = vae.quant_conv(vae.encoder(x))\n",
    "f_hat, _, _ = vae.quantize(f)\n",
    "\n",
    "\n",
    "image = vae.fhat_to_img(f_hat)\n",
    "\n",
    "# x = recon_B3HW.type(torch.float32).to(device)#.add_(-0.5).mul_(2)\n",
    "# f = vae.quant_conv(vae.encoder(x))\n",
    "# image = vae.fhat_to_img(f).add_(1).mul_(0.5)\n",
    "\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae.quantize.forward(f)\n",
    "# assert False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "patch_nums = (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n",
    "last_patch_id = -1\n",
    "\n",
    "quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "residual = f - f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(quant_pyramid)=0)\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(quant_pyramid)=})\")\n",
    "if quant_pyramid:\n",
    "    print(f\"{quant_pyramid[0].shape=}\")\n",
    "# assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the pyramid as the input for transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=-1, next_level=0\n",
      "next_token_map lvl current_level=-1 next_level=0: next_token_map.shape=torch.Size([16, 1, 1024])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_residual = var.autoregressive_single_step_prediction(\n",
    "    quant_pyramid=quant_pyramid, \n",
    "    f_hat=f_hat, \n",
    "    label_B=label_B,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 32, 16, 16])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_residual.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisation POC in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def get_downsampling_regularisation_loss(f_residual, patch_size):\n",
    "    downsampled = F.interpolate(f_residual, size=(patch_size, patch_size), mode='area', align_corners=False)\n",
    "\n",
    "    return downsampled.mean() # L1 regularisation\n",
    "\n",
    "\n",
    "def get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B):\n",
    "    predicted_residual = var.autoregressive_single_step_prediction(\n",
    "        quant_pyramid=quant_pyramid, \n",
    "        f_hat=f_hat, \n",
    "        label_B=label_B, \n",
    "        cfg=cfg, \n",
    "        top_k=900, \n",
    "        top_p=0.95)\n",
    "    \n",
    "    return F.mse_loss(predicted_residual, f_residual)\n",
    "\n",
    "def get_optimisation_loss(f, last_patch_id):\n",
    "    quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "    f_residual = f - f_hat\n",
    "\n",
    "    next_step_loss = get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B)\n",
    "\n",
    "    regularisation_losses = [\n",
    "        get_downsampling_regularisation_loss(f_residual, patch_size)\n",
    "        for patch_size in patch_nums[:last_patch_id]\n",
    "    ]\n",
    "\n",
    "    return next_step_loss + sum(regularisation_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_level=-1, next_level=0\n",
      "next_token_map lvl current_level=-1 next_level=0: next_token_map.shape=torch.Size([4, 1, 1024])\n",
      "\n",
      "coarsness_step: -1, loss: 0.39253973960876465\n",
      "current_level=0, next_level=1\n",
      "next_token_map lvl current_level=0 next_level=1: next_token_map.shape=torch.Size([4, 4, 1024])\n",
      "\n",
      "coarsness_step: 0, loss: 0.0009515469428151846\n",
      "current_level=1, next_level=2\n",
      "next_token_map lvl current_level=1 next_level=2: next_token_map.shape=torch.Size([4, 9, 1024])\n",
      "\n",
      "coarsness_step: 1, loss: 0.002446998842060566\n",
      "current_level=2, next_level=3\n",
      "next_token_map lvl current_level=2 next_level=3: next_token_map.shape=torch.Size([4, 16, 1024])\n",
      "\n",
      "coarsness_step: 2, loss: 0.004130854737013578\n",
      "current_level=3, next_level=4\n",
      "next_token_map lvl current_level=3 next_level=4: next_token_map.shape=torch.Size([4, 25, 1024])\n",
      "\n",
      "coarsness_step: 3, loss: 0.006683871150016785\n",
      "current_level=4, next_level=5\n",
      "next_token_map lvl current_level=4 next_level=5: next_token_map.shape=torch.Size([4, 36, 1024])\n",
      "\n",
      "coarsness_step: 4, loss: 0.009225272573530674\n",
      "current_level=5, next_level=6\n",
      "next_token_map lvl current_level=5 next_level=6: next_token_map.shape=torch.Size([4, 64, 1024])\n",
      "\n",
      "coarsness_step: 5, loss: 0.01421268004924059\n",
      "current_level=6, next_level=7\n",
      "next_token_map lvl current_level=6 next_level=7: next_token_map.shape=torch.Size([4, 100, 1024])\n",
      "\n",
      "coarsness_step: 6, loss: 0.02462838962674141\n",
      "current_level=7, next_level=8\n",
      "next_token_map lvl current_level=7 next_level=8: next_token_map.shape=torch.Size([4, 169, 1024])\n",
      "\n",
      "coarsness_step: 7, loss: 0.04235750064253807\n",
      "current_level=8, next_level=9\n",
      "next_token_map lvl current_level=8 next_level=9: next_token_map.shape=torch.Size([4, 256, 1024])\n",
      "\n",
      "coarsness_step: 8, loss: 0.10407813638448715\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = torch.rand([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (22, 562)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.1)\n",
    "\n",
    "steps_per_coarsness = 1\n",
    "\n",
    "for coarsness_step in range(-1, len((patch_nums)) - 1):\n",
    "    for i in range(steps_per_coarsness):\n",
    "        optimizer.zero_grad()\n",
    "        loss = get_optimisation_loss(f, coarsness_step)\n",
    "        patch_weight = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "        loss*= patch_weight * step_weight\n",
    "\n",
    "        if i % 500 == 0:\n",
    "            print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n",
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 3, 4, 5, 6, 8, 10, 13, 16)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.patch_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = vae.fhat_to_img(f).add_(1).mul_(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random coarsness step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimisation_loss(f, last_patch_id, f_hat=None, quant_pyramid=None):\n",
    "    if f_hat is None or quant_pyramid is None:\n",
    "        quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "    f_residual = f - f_hat\n",
    "\n",
    "#     predicted_residual = var.autoregressive_single_step_prediction(\n",
    "#         quant_pyramid=quant_pyramid, \n",
    "#         f_hat=f_hat, \n",
    "#         label_B=label_B, \n",
    "#         cfg=cfg, \n",
    "#         top_k=900, \n",
    "#         top_p=0.95)\n",
    "    \n",
    "#     # TODO add stability loss \n",
    "#     # Ensure f_residual scaled down to the \"current\" patch size should be close to zero (?)\n",
    "\n",
    "#     return F.l1_loss(predicted_residual, f_residual)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "def get_downsampling_regularisation_loss(f_residual, patch_size):\n",
    "    downsampled = F.interpolate(f_residual, size=(patch_size, patch_size), mode='area')\n",
    "\n",
    "    return F.mse_loss(downsampled, torch.zeros_like(downsampled)) # L1 regularisation\n",
    "\n",
    "\n",
    "def get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B):\n",
    "    predicted_residual = var.autoregressive_single_step_prediction(\n",
    "        quant_pyramid=quant_pyramid, \n",
    "        f_hat=f_hat, \n",
    "        label_B=label_B, \n",
    "        cfg=cfg, \n",
    "        top_k=900, \n",
    "        top_p=0.95)\n",
    "    \n",
    "    return F.mse_loss(predicted_residual, f_residual)\n",
    "\n",
    "def get_optimisation_loss(f, last_patch_id, f_hat=None, quant_pyramid=None, regularisation_weight=0):\n",
    "    if f_hat is None or quant_pyramid is None:\n",
    "        quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, last_patch_id)\n",
    "\n",
    "    f_residual = f - f_hat\n",
    "\n",
    "    next_step_loss = get_next_step_loss(quant_pyramid, f_residual, f_hat, label_B)\n",
    "\n",
    "    total_loss = next_step_loss\n",
    "\n",
    "\n",
    "    if regularisation_weight > 0:\n",
    "        regularisation_losses = [\n",
    "            get_downsampling_regularisation_loss(f_residual, patch_size)\n",
    "            for patch_size in patch_nums[:last_patch_id]\n",
    "        ]\n",
    "\n",
    "        total_loss += sum(regularisation_losses) * regularisation_weight\n",
    "\n",
    "        \n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_17319/3404125172.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=True, device=device)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:36<00:00, 27.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "# f = torch.randn([4, 32, 16, 16], requires_grad=True, device=device) \n",
    "# class_labels = (980, 437, 22, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "\n",
    "img = recon_B3HW.float().clone().clamp_(0, 1).to(device)\n",
    "f = torch.tensor(vae.quant_conv(vae.encoder(img).detach()), requires_grad=True, device=device)\n",
    "class_labels = (980, 980, 437, 437, 22, 22, 562, 562)  #@param {type:\"raw\"}\n",
    "\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.01)\n",
    "total_steps = 1000\n",
    "\n",
    "min_coarsness = len(patch_nums) - 2 #-1 #len(patch_nums) - 4\n",
    "max_coarsness = len(patch_nums) - 2 \n",
    "\n",
    "\n",
    "\n",
    "# quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, max_coarsness)\n",
    "quant_pyramid, f_hat = None, None\n",
    "\n",
    "for i in tqdm(range(total_steps), total=total_steps):\n",
    "    coarsness_step = np.random.randint(min_coarsness, max_coarsness + 1)\n",
    "\n",
    "    loss = get_optimisation_loss(f, coarsness_step, f_hat, quant_pyramid)\n",
    "\n",
    "    patch_grad_scale = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "    loss*= patch_grad_scale\n",
    "    \n",
    "    step_weight = (total_steps - i) / total_steps\n",
    "    loss*= step_weight\n",
    "\n",
    "    level_ratio = (coarsness_step + 2)/ (len(patch_nums) + 1) \n",
    "    level_weight = np.sqrt(level_ratio)\n",
    "    loss*= level_weight\n",
    "\n",
    "    # print(f'{level_ratio=}, {level_weight=}, {step_weight=}, {patch_grad_scale=}, {loss.item()=}')\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "f = f.detach()\n",
    "\n",
    "    # if i % 10 == 0:\n",
    "    #     print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(patch_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patch_sizes =  (1, 2, 3, 4, 5, 6, 8, 10, 13, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"patch_sizes = \", patch_nums)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed F-hat incremetal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coarsness_level: -1, loss: 0.0006090999025363968\n",
      "coarsness_level: 0, loss: 0.00017674557398780655\n",
      "coarsness_level: 1, loss: 7.816978732463561e-05\n",
      "coarsness_level: 2, loss: 0.000136359025294917\n",
      "coarsness_level: 3, loss: 0.00022669045838961596\n",
      "coarsness_level: 4, loss: 0.00021566336376152353\n",
      "coarsness_level: 5, loss: 0.0003899050076426731\n",
      "coarsness_level: 6, loss: 0.0004408489482989985\n",
      "coarsness_level: 7, loss: 0.0005911831812500716\n",
      "coarsness_level: 8, loss: 0.001166960721561788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "f = torch.zeros([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (22, 562)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.05)\n",
    "\n",
    "steps_per_coarsness = 500\n",
    "\n",
    "for coarsness_level in range(-1, len((patch_nums)) - 1):\n",
    "    losses = []\n",
    "\n",
    "    quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, coarsness_level)\n",
    "\n",
    "    f_clone = f.clone().detach()\n",
    "    \n",
    "    for i in range(steps_per_coarsness):\n",
    "        optimizer.zero_grad()\n",
    "        loss = get_optimisation_loss(f, coarsness_level, f_clone, quant_pyramid, regularisation_weight=0)\n",
    "        #patch_weight = 1 #patch_nums[coarsness_level]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        #step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "        #loss*= patch_weight * step_weight\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'coarsness_level: {coarsness_level}, loss: {np.mean(losses)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All at once optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|â–Ž         | 50/2000 [00:00<00:07, 252.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0002106762840412557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|â–‹         | 136/2000 [00:00<00:06, 274.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.1594459553853085e-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 221/2000 [00:00<00:07, 223.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00039762482629157603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|â–ˆâ–Œ        | 320/2000 [00:01<00:11, 151.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 7.952936721267179e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|â–ˆâ–ˆ        | 412/2000 [00:02<00:12, 127.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0012372417841106653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|â–ˆâ–ˆâ–Œ       | 510/2000 [00:03<00:15, 93.53it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.0016484935767948627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|â–ˆâ–ˆâ–ˆ       | 610/2000 [00:04<00:16, 84.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.00352624268271029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 712/2000 [00:05<00:18, 68.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.003063205862417817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 811/2000 [00:07<00:19, 62.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.004616558086127043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 909/2000 [00:09<00:20, 54.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.006852190941572189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1005/2000 [00:11<00:19, 51.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.007261977531015873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1106/2000 [00:13<00:20, 44.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.012157270684838295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 1206/2000 [00:15<00:18, 42.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.016481220722198486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1305/2000 [00:18<00:18, 37.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.015166565775871277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 1405/2000 [00:20<00:16, 35.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.02793954312801361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 1505/2000 [00:24<00:15, 31.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.025798745453357697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1605/2000 [00:27<00:13, 28.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.04659782350063324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1704/2000 [00:31<00:12, 23.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.04217388853430748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 1803/2000 [00:35<00:08, 22.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.10323582589626312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1904/2000 [00:40<00:05, 18.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.14244210720062256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2000/2000 [00:45<00:00, 43.48it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "f = torch.zeros([2, 32, 16, 16], requires_grad=True, device=device)\n",
    "\n",
    "class_labels = (980, 437)  #@param {type:\"raw\"}\n",
    "label_B: torch.LongTensor = torch.tensor(class_labels, device=device)\n",
    "\n",
    "optimizer = torch.optim.Adam([f], lr=0.05)\n",
    "\n",
    "total_steps = 2000\n",
    "\n",
    "for i in tqdm(range(total_steps), total=total_steps):\n",
    "    loss = 0\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    final_patch = int(i/total_steps * len(patch_nums)) \n",
    "\n",
    "    for coarsness_level in range(-1, final_patch):\n",
    "        patch_weight = patch_nums[coarsness_level+1]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "        loss += get_optimisation_loss(f, coarsness_level, regularisation_weight=0) * patch_weight\n",
    "        \n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'loss: {loss.item()}')\n",
    "\n",
    "    #step_weight = (total_steps - i) / total_steps\n",
    "\n",
    "\n",
    "\n",
    "    #loss*= step_weight\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# for coarsness_step in range(-1, len((patch_nums)) - 1):\n",
    "#     for i in range(steps_per_coarsness):\n",
    "#         optimizer.zero_grad()\n",
    "#         loss = get_optimisation_loss(f, coarsness_step)\n",
    "#         patch_weight = patch_nums[coarsness_step]**2 / patch_nums[len(patch_nums)-1]**2\n",
    "#         step_weight = (steps_per_coarsness - i) /steps_per_coarsness\n",
    "        \n",
    "#         loss*= patch_weight * step_weight\n",
    "\n",
    "#         if i % 500 == 0:\n",
    "#             print(f'coarsness_step: {coarsness_step}, loss: {loss.item()}')\n",
    "\n",
    "        \n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "kf.service.services: KApplicationTrader: mimeType \"x-scheme-handler/file\" not found\n",
      "kf.i18n.kuit: \"Unknown subcue ':whatsthis,' in UI marker in context {@info:whatsthis, %1 the action's text}.\"\n",
      "org.kde.kdegraphics.gwenview.lib: Unresolved raw mime type  \"image/x-samsung-srw\"\n"
     ]
    }
   ],
   "source": [
    "image = vae.fhat_to_img(f.detach()).clamp_(0, 1)\n",
    "chw = torchvision.utils.make_grid(image, nrow=8, padding=0, pad_value=1.0)\n",
    "chw = chw.permute(1, 2, 0).mul_(255).cpu().numpy()\n",
    "chw = PImage.fromarray(chw.astype(np.uint8))\n",
    "chw.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAR prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coarsness_level in range(-1, len((patch_nums)) - 1):\n",
    "    losses = []\n",
    "\n",
    "    quant_pyramid, f_hat = vae.quantize.f_to_quant_pyramid_and_f_hat(f, patch_nums, coarsness_level)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-var",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
